{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to spaCy/DaCy and Named Entity Recognition (NER)\n",
    "This notebook is an introduction to the spaCy/DaCy universe, and will introduce some of its basic functionality such as  extracting named entities (e.g. people, places, locations) from text.\n",
    "\n",
    "In this notebook we will focus on small examples, but the methods carry over to the data that we have prepared for you on UCloud. \n",
    "\n",
    "This notebook is mainly meant as a reference to consult when working on the next task. Read through it, run the code, and try to think of ways these methods might be useful for gaining insights from the web/Twitter data. The next notebook contains exercises specific to your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why NER?\n",
    "\n",
    "Named Entity Recognition (NER) is the task of identifying named entities in a text. A named entity is a “real-world object” that’s assigned a name - for example, a person, a country, a product or a book title.\n",
    "NER is extremely usable for a wide range of tasks:\n",
    "1. Anonymising documents (replacing named entities with a pseudonym)\n",
    "2. Information extraction: finding important actors/entities within a document. This could be used to e.g. automatically link to an employee's profile or to create location tags.\n",
    "3. Categorizing documents based on the occurence of certain entities\n",
    "\n",
    "## NER in Danish\n",
    "\n",
    "There are multiple tools for Danish NER. The fastest uses the [spaCy](https://spacy.io/) library, and the most accurate uses [DaCy](https://github.com/centre-for-humanities-computing/DaCy), which was developed here at CHC.\n",
    "\n",
    "\n",
    "### Getting started\n",
    "\n",
    "First off, we need to import the libraries that we intend to use. Let's load spaCy and DaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dacy\n",
      "  Using cached dacy-2.7.1-py3-none-any.whl (54 kB)\n",
      "Collecting spacy-experimental>=0.6.2\n",
      "  Using cached spacy_experimental-0.6.2-cp310-cp310-macosx_10_9_x86_64.whl (739 kB)\n",
      "Collecting pandas<2.0.0,>=1.0.0\n",
      "  Using cached pandas-1.5.3-cp310-cp310-macosx_10_9_x86_64.whl (12.0 MB)\n",
      "Collecting wasabi<0.11.0,>=0.8.2\n",
      "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting tqdm<4.66.0,>=4.42.1\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting spacy-wrap<1.5.0,>=1.4.1\n",
      "  Using cached spacy_wrap-1.4.3-py3-none-any.whl (20 kB)\n",
      "Collecting spacy[transformers]<3.6.0,>=3.2.0\n",
      "  Using cached spacy-3.5.3-cp310-cp310-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.10/site-packages (from pandas<2.0.0,>=1.0.0->dacy) (2.8.2)\n",
      "Collecting numpy>=1.21.0\n",
      "  Using cached numpy-1.25.0-cp310-cp310-macosx_10_9_x86_64.whl (20.1 MB)\n",
      "Collecting thinc<8.2.0,>=8.0.13\n",
      "  Using cached thinc-8.1.10-cp310-cp310-macosx_10_9_x86_64.whl (859 kB)\n",
      "Collecting spacy-transformers<1.3.0,>=1.2.1\n",
      "  Using cached spacy_transformers-1.2.5-cp310-cp310-macosx_10_9_x86_64.whl (179 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp310-cp310-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp310-cp310-macosx_10_9_x86_64.whl (106 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Using cached pydantic-1.10.9-cp310-cp310-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy[transformers]<3.6.0,>=3.2.0->dacy) (23.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp310-cp310-macosx_10_9_x86_64.whl (32 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.6-cp310-cp310-macosx_10_9_x86_64.whl (491 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy[transformers]<3.6.0,>=3.2.0->dacy) (67.2.0)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Using cached typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.0.0->dacy) (1.16.0)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.1.0-cp310-cp310-macosx_10_9_x86_64.whl (124 kB)\n",
      "Collecting transformers<4.31.0,>=3.4.0\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Collecting torch>=1.8.0\n",
      "  Using cached torch-2.0.1-cp310-none-macosx_10_9_x86_64.whl (143.4 MB)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
      "  Using cached spacy_alignments-0.9.0-cp310-cp310-macosx_10_9_x86_64.whl (319 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.9-cp310-cp310-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-macosx_10_9_x86_64.whl (13 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.6.3-cp310-cp310-macosx_10_9_x86_64.whl (294 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Using cached huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp310-cp310-macosx_10_9_x86_64.whl (197 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.3.1-cp310-cp310-macosx_10_11_x86_64.whl (400 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: wasabi, tokenizers, safetensors, pytz, mpmath, cymem, urllib3, typing-extensions, tqdm, sympy, spacy-loggers, spacy-legacy, spacy-alignments, smart-open, regex, pyyaml, numpy, networkx, murmurhash, MarkupSafe, langcodes, idna, fsspec, filelock, click, charset-normalizer, certifi, catalogue, typer, srsly, requests, pydantic, preshed, pandas, jinja2, blis, torch, pathy, huggingface-hub, confection, transformers, thinc, spacy, spacy-transformers, spacy-experimental, spacy-wrap, dacy\n",
      "Successfully installed MarkupSafe-2.1.3 blis-0.7.9 catalogue-2.0.8 certifi-2023.5.7 charset-normalizer-3.1.0 click-8.1.3 confection-0.0.4 cymem-2.0.7 dacy-2.7.1 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.15.1 idna-3.4 jinja2-3.1.2 langcodes-3.3.0 mpmath-1.3.0 murmurhash-1.0.9 networkx-3.1 numpy-1.25.0 pandas-1.5.3 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.9 pytz-2023.3 pyyaml-6.0 regex-2023.6.3 requests-2.31.0 safetensors-0.3.1 smart-open-6.3.0 spacy-3.5.3 spacy-alignments-0.9.0 spacy-experimental-0.6.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 spacy-transformers-1.2.5 spacy-wrap-1.4.3 srsly-2.4.6 sympy-1.12 thinc-8.1.10 tokenizers-0.13.3 torch-2.0.1 tqdm-4.65.0 transformers-4.30.2 typer-0.7.0 typing-extensions-4.6.3 urllib3-2.0.3 wasabi-0.10.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/au554730/Desktop/Projects/au-it-danish-nlp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dacy\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the language models, we have to load them. Let's see an example with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a Danish spacy model\n",
    "nlp = spacy.load(\"da_core_news_lg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model is as simple as supplying a text to the `nlp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Joe Biden omtalte Kinas præsident som diktator: 'Nu kommer den rigtige test af, om de vil forbedre forholdet'\" \n",
    "doc = nlp(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use spaCy/daCy to analyse the text. For instance, we can look at the individual sentences in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe Biden omtalte Kinas præsident som diktator: '\n",
      "Nu kommer den rigtige test af, om de vil forbedre forholdet'\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or find the named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe Biden PER\n",
      "Kinas LOC\n"
     ]
    }
   ],
   "source": [
    "for named_entity in doc.ents:\n",
    "    print(named_entity.text, named_entity.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the named entities using the built-in visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Joe Biden\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " omtalte \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kinas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " præsident som diktator: 'Nu kommer den rigtige test af, om de vil forbedre forholdet'</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-grained NER with DaCy\n",
    "While the model for named entity recognition in spaCy is fast, it's limited to only people (PER), locations (LOC), and organizations (ORG). The DaCy model allows us to look at more fine-grained entities as illustrated in the table below:\n",
    "\n",
    "|  Tag        |             Description                                         | \n",
    "| -------- | ---------------------------------------------------- | \n",
    "| PERSON   | People, including fictional                          | \n",
    "| NORP     | Nationalities or religious or political groups       | \n",
    "| FACILITY | Building, airports, highways, bridges, etc.          | \n",
    "| ORGANIZATION | Companies, agencies, institutions, etc.              | \n",
    "| GPE      | Countries, cities, states.                           | \n",
    "| LOCATION | Non-GPE locations, mountain ranges, bodies of water  | \n",
    "| PRODUCT  | Vehicles, weapons, foods, etc. (not services)        | \n",
    "| EVENT    | Named hurricanes, battles, wars, sports events, etc. | \n",
    "| WORK OF ART | Titles of books, songs, etc.                         | \n",
    "| LAW      | Named documents made into laws                       | \n",
    "| LANGUAGE | Any named language                                   | \n",
    "     \n",
    "As well as annotations for the following concepts: \n",
    "\n",
    "\n",
    "|   Tag       |   Description                                         | \n",
    "| -------- | ------------------------------------------- | \n",
    "| DATE     | Absolute or relative dates or periods       | \n",
    "| TIME     | Times smaller than a day                    | \n",
    "| PERCENT  | Percentage (including '\\*'\\%)                | \n",
    "| MONEY    | Monetary values, including unit             | \n",
    "| QUANTITY | Measurements, as of weight or distance      | \n",
    "| ORDINAL  | \"first\", \"second\"                           | \n",
    "| CARDINAL | Numerals that do no fall under another type | \n",
    "\n",
    "\n",
    "Let's try to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting da-dacy-small-trf==any\n",
      "  Downloading https://huggingface.co/chcaa/da_dacy_small_trf/resolve/0eadea074d5f637e76357c46bbd56451471d0154/da_dacy_small_trf-any-py3-none-any.whl (101.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.3/101.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting da-dacy-small-ner-fine-grained==any\n",
      "  Downloading https://huggingface.co/chcaa/da_dacy_small_ner_fine_grained/resolve/43fedc5a1b1c1d193f461d13225f217f2ced507d/da_dacy_small_ner_fine_grained-any-py3-none-any.whl (82.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.ner.EntityRecognizer at 0x12ca81930>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the small dacy model excluding the NER component\n",
    "dacy_nlp = dacy.load(\"small\", exclude=[\"ner\"])\n",
    "\n",
    "# add the ner component from the fine-grained model\n",
    "dacy_nlp.add_pipe(\"dacy/ner-fine-grained\", config={\"size\": \"small\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Denne model samt \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " andre blev trænet \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    d. 7. marts\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " af \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Center for Humantities Computing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORGANIZATION</span>\n",
       "</mark>\n",
       " i \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aarhus kommune\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = dacy_nlp(\"Denne model samt 3 andre blev trænet d. 7. marts af Center for Humantities Computing i Aarhus kommune\")\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running spaCy/DaCy on multiple texts\n",
    "To analyze multiple texts at once, you can use the `nlp.pipe` method. Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dacy_nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m texts \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHer er det 1. tekststykke. Det er kort, og uden personer.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHer er det 2. tekststykke. Det er lidt længere, og indeholder en person. Vi kunne kalde ham Kristoffer.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0m docs \u001b[39m=\u001b[39m dacy_nlp\u001b[39m.\u001b[39mpipe(texts)\n\u001b[1;32m      7\u001b[0m \u001b[39m# iterate over the documents one by one\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dacy_nlp' is not defined"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Her er det 1. tekststykke. Det er kort, og uden personer.\",\n",
    "    \"Her er det 2. tekststykke. Det er lidt længere, og indeholder en person. Vi kunne kalde ham Kristoffer.\",\n",
    "]\n",
    "\n",
    "docs = dacy_nlp.pipe(texts)\n",
    "# iterate over the documents one by one\n",
    "for doc in docs:\n",
    "    print(\"First document!\")\n",
    "    displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the act of grouping together the inflected forms of a word so they can be analysed as a single item. For example, the verb “to run” has the base form “run”, and the verb “ran” has the base form “run”.\n",
    "\n",
    "Lemmatization is for example used for text normalization before training a machine learning model to reduce the number of unique tokens in the training data. Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaCy DaCy\n",
      "er være\n",
      "en en\n",
      "hurtig hurtig\n",
      "og og\n",
      "effektiv effektiv\n",
      "pipeline pipeline\n",
      "til til\n",
      "dansk dansk\n",
      "sprogprocessering sprogprocessering\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Normalisering af tekst kan være en god idé.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other linguistic features\n",
    "\n",
    "SpaCy/DaCy is not limited to extracting NER or doing lemmatization. You can perform many complex linguistic analysis, such as investigating part-of-speech tags, or the dependency relations between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denne DET det model\n",
      "model NOUN nsubj trænet\n",
      "samt CCONJ dep model\n",
      "3 NUM dep model\n",
      "andre PRON nmod 3\n",
      "blev AUX aux trænet\n",
      "trænet VERB ROOT trænet\n",
      "d. ADV advmod trænet\n",
      "7. ADJ amod marts\n",
      "marts NOUN obl d.\n",
      "af ADP case Center\n",
      "Center NOUN obl trænet\n",
      "for ADP dep Humantities\n",
      "Humantities PROPN nmod Center\n",
      "Computing PROPN nmod:poss Humantities\n",
      "i ADP case Aarhus\n",
      "Aarhus PROPN nmod Center\n",
      "kommune NOUN flat Aarhus\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples, check out the [DaCy tutorials](https://centre-for-humanities-computing.github.io/DaCy/tutorials.html) or the [spaCy 101](https://spacy.io/usage/spacy-101)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
